import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from datetime import datetime, timedelta
import io
import warnings
warnings.filterwarnings('ignore')

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.vector_ar.var_model import VAR
from statsmodels.tsa.stattools import grangercausalitytests
from statsmodels.tsa.vector_ar.irf import IRAnalysis
from pmdarima import auto_arima
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="æ™‚ç³»åˆ—äºˆæ¸¬ã‚¢ãƒ—ãƒª",
    page_icon="ğŸ“ˆ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆç’°å¢ƒã«å¿œã˜ã¦å¤‰æ›´ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™ï¼‰
plt.rcParams['font.family'] = 'Meiryo', 'MS Gothic', 'Arial', 'sans-serif'

# ã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«
st.title('ğŸ“ˆ æ™‚ç³»åˆ—äºˆæ¸¬ã‚¢ãƒ—ãƒª')
st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã¯ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦è¤‡æ•°ã®äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨ã—ã€çµæœã‚’æ¯”è¼ƒãƒ»å¯è¦–åŒ–ã—ã¾ã™ã€‚
å¯¾å¿œãƒ¢ãƒ‡ãƒ«: ARIMA, SARIMAX, VAR, LSTM
""")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'df' not in st.session_state:
    st.session_state.df = None
if 'models' not in st.session_state:
    st.session_state.models = {}
if 'results' not in st.session_state:
    st.session_state.results = {}

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
st.sidebar.header('è¨­å®š')

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.sidebar.file_uploader(
    "Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", 
    type=["xlsx", "xls"]
)

def load_data(file):
    """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    if file is not None:
        try:
            df = pd.read_excel(file)
            return df
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None
    return None

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
if uploaded_file is not None:
    df = load_data(uploaded_file)
    if df is not None:
        st.session_state.df = df

# ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚ŒãŸã‚‰è¨­å®šã‚’è¡¨ç¤º
if st.session_state.df is not None:
    df = st.session_state.df
    
    # ã‚«ãƒ©ãƒ é¸æŠ
    st.sidebar.subheader('ãƒ‡ãƒ¼ã‚¿è¨­å®š')
    date_col = st.sidebar.selectbox('æ—¥ä»˜åˆ—ã‚’é¸æŠ', df.columns, index=0)
    target_col = st.sidebar.selectbox('ç›®çš„å¤‰æ•°åˆ—ã‚’é¸æŠ', df.columns, index=min(1, len(df.columns)-1))
    
    # æ—¥ä»˜åˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š
    try:
        df[date_col] = pd.to_datetime(df[date_col])
        df = df.set_index(date_col)
        df.sort_index(inplace=True)
    except Exception as e:
        st.error(f"æ—¥ä»˜åˆ—ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    # æ™‚ç³»åˆ—ã®é »åº¦è¨­å®š
    freq_options = ['è‡ªå‹•æ¤œå‡º', 'æ—¥æ¬¡ (D)', 'é€±æ¬¡ (W)', 'æœˆæ¬¡ (M)', 'å››åŠæœŸ (Q)', 'å¹´æ¬¡ (Y)']
    freq_setting = st.sidebar.selectbox('æ™‚ç³»åˆ—ã®é »åº¦', freq_options, index=0)
    
    # å¤–éƒ¨å¤‰æ•°ã®é¸æŠï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰
    exog_cols = st.sidebar.multiselect(
        'å¤–éƒ¨å¤‰æ•°ã‚’é¸æŠï¼ˆä»»æ„ï¼‰',
        [col for col in df.columns if col != target_col]
    )
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æœŸé–“ã®è¨­å®š
    st.sidebar.subheader('åˆ†ææœŸé–“è¨­å®š')
    train_start = st.sidebar.date_input(
        'å­¦ç¿’é–‹å§‹æ—¥',
        value=df.index.min().to_pydatetime().date(),
        min_value=df.index.min().to_pydatetime().date(),
        max_value=df.index.max().to_pydatetime().date()
    )
    
    train_end = st.sidebar.date_input(
        'å­¦ç¿’çµ‚äº†æ—¥',
        value=df.index.max().to_pydatetime().date() - timedelta(days=30),
        min_value=df.index.min().to_pydatetime().date(),
        max_value=df.index.max().to_pydatetime().date()
    )
    
    # äºˆæ¸¬æœŸé–“ã®è¨­å®š
    forecast_steps = st.sidebar.number_input(
        'äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—æ•°',
        min_value=1,
        max_value=365,
        value=30,
        step=1
    )
    
    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    st.sidebar.subheader('ãƒ¢ãƒ‡ãƒ«è¨­å®š')
    use_arima = st.sidebar.checkbox('ARIMA', value=True)
    use_sarimax = st.sidebar.checkbox('SARIMAX', value=len(exog_cols) > 0)
    use_var = st.sidebar.checkbox('VAR', value=len(exog_cols) > 0)
    use_lstm = st.sidebar.checkbox('LSTM', value=True)
    
    # LSTMå›ºæœ‰ã®è¨­å®š
    if use_lstm:
        st.sidebar.subheader('LSTMè¨­å®š')
        lstm_timesteps = st.sidebar.number_input(
            'ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆå±¥æ­´ã®é•·ã•ï¼‰',
            min_value=1,
            max_value=100,
            value=10,
            step=1
        )
        
        lstm_epochs = st.sidebar.number_input(
            'ã‚¨ãƒãƒƒã‚¯æ•°',
            min_value=1,
            max_value=1000,
            value=100,
            step=10
        )
        
        lstm_units = st.sidebar.number_input(
            'LSTMãƒ¦ãƒ‹ãƒƒãƒˆæ•°',
            min_value=1,
            max_value=256,
            value=50,
            step=10
        )
        
        lstm_dropout = st.sidebar.slider(
            'Dropoutç‡',
            min_value=0.0,
            max_value=0.5,
            value=0.2,
            step=0.05
        )
        
        lstm_scaler = st.sidebar.selectbox(
            'ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ–¹æ³•',
            ['MinMaxScaler', 'StandardScaler'],
            index=0
        )
    
    # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
    train_data = df[train_start:train_end]
    
    # ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œãƒœã‚¿ãƒ³
    if st.sidebar.button('ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œ'):
        with st.spinner('ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œä¸­...'):
            st.session_state.models = {}
            st.session_state.results = {}
            
            # äºˆæ¸¬ç”¨ã®æ—¥ä»˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
            last_date = train_data.index[-1]
            if freq_setting == 'è‡ªå‹•æ¤œå‡º':
                freq = pd.infer_freq(df.index)
                if freq is None:
                    freq = 'D'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æ—¥æ¬¡
            else:
                freq = freq_setting.split(' ')[1][1:-1]  # 'æ—¥æ¬¡ (D)' ã‹ã‚‰ 'D' ã‚’æŠ½å‡º
            
            future_dates = pd.date_range(
                start=last_date + pd.Timedelta(days=1),
                periods=forecast_steps,
                freq=freq
            )
            
            # äºˆæ¸¬çµæœã‚’æ ¼ç´ã™ã‚‹DataFrame
            forecast_df = pd.DataFrame(index=future_dates)
            
            # ARIMAãƒ¢ãƒ‡ãƒ«
            if use_arima:
                try:
                    with st.spinner('ARIMAãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œä¸­...'):
                        # è‡ªå‹•ARIMAã§æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢
                        model_arima = auto_arima(
                            train_data[target_col],
                            seasonal=False,
                            trace=False,
                            error_action='ignore',
                            suppress_warnings=True
                        )
                        
                        # ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
                        model_fit = model_arima.fit(train_data[target_col])
                        
                        # äºˆæ¸¬
                        forecast = model_fit.predict(n_periods=forecast_steps)
                        forecast_df['ARIMA'] = forecast.values
                        
                        # ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’ä¿å­˜
                        st.session_state.models['ARIMA'] = model_fit
                        st.session_state.results['ARIMA'] = {
                            'forecast': forecast_df[['ARIMA']],
                            'metrics': {},
                            'model_info': str(model_fit.summary())
                        }
                        
                except Exception as e:
                    st.error(f'ARIMAãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}')
            
            # SARIMAXãƒ¢ãƒ‡ãƒ«ï¼ˆå¤–éƒ¨å¤‰æ•°ã‚ã‚Šï¼‰
            if use_sarimax and exog_cols:
                try:
                    with st.spinner('SARIMAXãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œä¸­...'):
                        # è‡ªå‹•SARIMAXã§æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢
                        model_sarimax = auto_arima(
                            train_data[target_col],
                            X=train_data[exog_cols],
                            seasonal=True,
                            trace=False,
                            error_action='ignore',
                            suppress_warnings=True
                        )
                        
                        # ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
                        model_fit = SARIMAX(
                            train_data[target_col],
                            exog=train_data[exog_cols],
                            order=model_sarimax.order,
                            seasonal_order=model_sarimax.seasonal_order
                        ).fit(disp=False)
                        
                        # äºˆæ¸¬ç”¨ã®å¤–éƒ¨å¤‰æ•°ã‚’æº–å‚™
                        # å˜ç´”ã«æœ€å¾Œã®å€¤ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ä½¿ç”¨ï¼ˆå®Ÿéš›ã®ã‚¢ãƒ—ãƒªã§ã¯ã‚ˆã‚Šé«˜åº¦ãªå‡¦ç†ãŒå¿…è¦ï¼‰
                        exog_forecast = pd.DataFrame(
                            np.tile(train_data[exog_cols].values[-1], (forecast_steps, 1)),
                            columns=exog_cols,
                            index=future_dates
                        )
                        
                        # äºˆæ¸¬
                        forecast = model_fit.get_forecast(steps=forecast_steps, exog=exog_forecast)
                        forecast_df['SARIMAX'] = forecast.predicted_mean
                        
                        # ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’ä¿å­˜
                        st.session_state.models['SARIMAX'] = model_fit
                        st.session_state.results['SARIMAX'] = {
                            'forecast': forecast_df[['SARIMAX']],
                            'metrics': {},
                            'model_info': str(model_fit.summary())
                        }
                        
                except Exception as e:
                    st.error(f'SARIMAXãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}')
            
            # VARãƒ¢ãƒ‡ãƒ«ï¼ˆå¤šå¤‰é‡æ™‚ç³»åˆ—ï¼‰
            if use_var and len(exog_cols) > 0:
                try:
                    with st.spinner('VARãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œä¸­...'):
                        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
                        var_data = train_data[[target_col] + exog_cols].dropna()
                        
                        # æœ€é©ãªãƒ©ã‚°ã‚’é¸æŠ
                        # å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã§ã¯ã‚ˆã‚Šé«˜åº¦ãªæ–¹æ³•ã§é¸æŠã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
                        model_var = VAR(var_data)
                        results = model_var.fit()
                        
                        # äºˆæ¸¬
                        lag_order = results.k_ar
                        forecast_input = var_data.values[-lag_order:]
                        forecast = results.forecast(forecast_input, steps=forecast_steps)
                        
                        # äºˆæ¸¬çµæœã‚’DataFrameã«å¤‰æ›
                        forecast_df['VAR'] = forecast[:, 0]  # ç›®çš„å¤‰æ•°ã®äºˆæ¸¬ã®ã¿ã‚’å–å¾—
                        
                        # ã‚°ãƒ¬ãƒ³ã‚¸ãƒ£ãƒ¼å› æœæ€§æ¤œå®š
                        gc_results = {}
                        for col in exog_cols:
                            try:
                                gc_test = grangercausalitytests(var_data[[target_col, col]], maxlag=min(5, len(var_data)//3), verbose=False)
                                p_values = [round(gc_test[i+1][0]['ssr_ftest'][1], 4) for i in range(len(gc_test))]
                                gc_results[col] = {
                                    'p_values': p_values,
                                    'min_p_value': min(p_values)
                                }
                            except:
                                pass
                        
                        # ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹å¿œç­”é–¢æ•°
                        irf = results.irf(10)
                        
                        # ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’ä¿å­˜
                        st.session_state.models['VAR'] = results
                        st.session_state.results['VAR'] = {
                            'forecast': forecast_df[['VAR']],
                            'metrics': {},
                            'model_info': str(results.summary()),
                            'granger_causality': gc_results,
                            'irf': irf
                        }
                        
                except Exception as e:
                    st.error(f'VARãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}')
            
            # LSTMãƒ¢ãƒ‡ãƒ«
            if use_lstm:
                try:
                    with st.spinner('LSTMãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œä¸­...'):
                        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
                        features = [target_col] + exog_cols if exog_cols else [target_col]
                        lstm_data = train_data[features].copy()
                        
                        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                        if lstm_scaler == 'MinMaxScaler':
                            scaler = MinMaxScaler()
                        else:
                            scaler = StandardScaler()
                            
                        scaled_data = scaler.fit_transform(lstm_data)
                        
                        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
                        X, y = [], []
                        for i in range(len(scaled_data) - lstm_timesteps):
                            X.append(scaled_data[i:(i + lstm_timesteps)])
                            y.append(scaled_data[i + lstm_timesteps, 0])  # ç›®çš„å¤‰æ•°ã®ã¿
                            
                        X = np.array(X)
                        y = np.array(y)
                        
                        # ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
                        model = Sequential([
                            LSTM(units=lstm_units, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),
                            Dropout(lstm_dropout),
                            LSTM(units=lstm_units, return_sequences=False),
                            Dropout(lstm_dropout),
                            Dense(1)
                        ])
                        
                        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
                        
                        # ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
                        history = model.fit(
                            X, y,
                            epochs=lstm_epochs,
                            batch_size=32,
                            validation_split=0.1,
                            verbose=0
                        )
                        
                        # äºˆæ¸¬ã®ãŸã‚ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
                        last_sequence = scaled_data[-lstm_timesteps:]
                        future_predictions = []
                        
                        for _ in range(forecast_steps):
                            # äºˆæ¸¬
                            next_step = model.predict(last_sequence.reshape(1, lstm_timesteps, len(features)), verbose=0)
                            future_predictions.append(next_step[0, 0])
                            
                            # äºˆæ¸¬å€¤ã‚’å…¥åŠ›ã«è¿½åŠ ï¼ˆè‡ªå·±å›å¸°çš„ã«äºˆæ¸¬ï¼‰
                            new_row = np.zeros(len(features))
                            new_row[0] = next_step[0, 0]  # ç›®çš„å¤‰æ•°ã®äºˆæ¸¬å€¤
                            
                            # å¤–éƒ¨å¤‰æ•°ãŒã‚ã‚‹å ´åˆã¯ã€æœ€å¾Œã®å€¤ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆå®Ÿéš›ã®ã‚¢ãƒ—ãƒªã§ã¯ã‚ˆã‚Šé«˜åº¦ãªå‡¦ç†ãŒå¿…è¦ï¼‰
                            if len(features) > 1:
                                new_row[1:] = last_sequence[-1, 1:]
                                
                            last_sequence = np.vstack([last_sequence[1:], new_row])
                        
                        # äºˆæ¸¬çµæœã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™
                        dummy_array = np.zeros((len(future_predictions), len(features)))
                        dummy_array[:, 0] = future_predictions
                        forecast_values = scaler.inverse_transform(dummy_array)[:, 0]
                        
                        # äºˆæ¸¬çµæœã‚’DataFrameã«æ ¼ç´
                        forecast_df['LSTM'] = forecast_values
                        
                        # ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’ä¿å­˜
                        st.session_state.models['LSTM'] = model
                        st.session_state.results['LSTM'] = {
                            'forecast': forecast_df[['LSTM']],
                            'metrics': {},
                            'training_history': history.history,
                            'model_summary': []
                        }
                        
                        # ãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
                        stringlist = []
                        model.summary(print_fn=lambda x: stringlist.append(x))
                        st.session_state.results['LSTM']['model_summary'] = '\n'.join(stringlist)
                        
                except Exception as e:
                    st.error(f'LSTMãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}')
            
            # äºˆæ¸¬çµæœã‚’ä¿å­˜
            st.session_state.forecast_df = forecast_df
            st.session_state.train_data = train_data
            
            # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
            if len(forecast_df.columns) > 0:
                st.success('ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸï¼')
            else:
                st.warning('å®Ÿè¡Œå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚')
    
    # çµæœã®è¡¨ç¤º
    if 'forecast_df' in st.session_state and not st.session_state.forecast_df.empty:
        st.subheader('äºˆæ¸¬çµæœ')
        
        # äºˆæ¸¬çµæœã®ãƒ—ãƒ­ãƒƒãƒˆ
        fig = go.Figure()
        
        # å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ­ãƒƒãƒˆ
        fig.add_trace(go.Scatter(
            x=train_data.index,
            y=train_data[target_col],
            mode='lines',
            name='å®Ÿç¸¾',
            line=dict(color='blue')
        ))
        
        # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        for model_name in st.session_state.forecast_df.columns:
            fig.add_trace(go.Scatter(
                x=st.session_state.forecast_df.index,
                y=st.session_state.forecast_df[model_name],
                mode='lines',
                name=f'{model_name} äºˆæ¸¬',
                line=dict(dash='dash')
            ))
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®è¨­å®š
        fig.update_layout(
            title=f'{target_col} ã®æ™‚ç³»åˆ—äºˆæ¸¬',
            xaxis_title='æ—¥ä»˜',
            yaxis_title=target_col,
            legend_title='å‡¡ä¾‹',
            template='plotly_white',
            hovermode='x unified',
            height=600
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ã‚¿ãƒ–ã§å„ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã‚’è¡¨ç¤º
        tab_titles = list(st.session_state.results.keys())
        tabs = st.tabs(tab_titles)
        
        for i, (model_name, tab) in enumerate(zip(tab_titles, tabs)):
            with tab:
                result = st.session_state.results[model_name]
                
                # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º
                st.subheader(f'{model_name} ãƒ¢ãƒ‡ãƒ«æƒ…å ±')
                
                if model_name == 'LSTM':
                    # LSTMã®ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
                    st.text('ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:')
                    st.code(result['model_summary'], language='text')
                    
                    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å±¥æ­´ã®ãƒ—ãƒ­ãƒƒãƒˆ
                    st.subheader('ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å±¥æ­´')
                    fig_loss = go.Figure()
                    fig_loss.add_trace(go.Scatter(
                        y=result['training_history']['loss'],
                        mode='lines',
                        name='Training Loss'
                    ))
                    if 'val_loss' in result['training_history']:
                        fig_loss.add_trace(go.Scatter(
                            y=result['training_history']['val_loss'],
                            mode='lines',
                            name='Validation Loss'
                        ))
                    fig_loss.update_layout(
                        title='ãƒ¢ãƒ‡ãƒ«ã®æå¤±',
                        xaxis_title='Epoch',
                        yaxis_title='Loss',
                        template='plotly_white'
                    )
                    st.plotly_chart(fig_loss, use_container_width=True)
                    
                elif model_name == 'VAR':
                    # VARãƒ¢ãƒ‡ãƒ«ã®ã‚°ãƒ¬ãƒ³ã‚¸ãƒ£ãƒ¼å› æœæ€§æ¤œå®šçµæœã‚’è¡¨ç¤º
                    st.subheader('ã‚°ãƒ¬ãƒ³ã‚¸ãƒ£ãƒ¼å› æœæ€§æ¤œå®š')
                    gc_data = []
                    for col, gc_result in result['granger_causality'].items():
                        gc_data.append({
                            'å¤‰æ•°': col,
                            'æœ€å°på€¤': f"{gc_result['min_p_value']:.4f}",
                            'å› æœé–¢ä¿‚ã®æœ‰ç„¡': 'ã‚ã‚Š' if gc_result['min_p_value'] < 0.05 else 'ãªã—'
                        })
                    
                    if gc_data:
                        st.table(pd.DataFrame(gc_data))
                    
                    # ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹å¿œç­”é–¢æ•°ã®ãƒ—ãƒ­ãƒƒãƒˆ
                    st.subheader('ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹å¿œç­”é–¢æ•°')
                    try:
                        irf_fig = result['irf'].plot(plot_stderr=False, figsize=(12, 8))
                        st.pyplot(irf_fig[0].figure)
                    except Exception as e:
                        st.error(f'ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹å¿œç­”é–¢æ•°ã®ãƒ—ãƒ­ãƒƒãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}')
                    
                else:
                    # ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
                    st.text('ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒªãƒ¼:')
                    st.text(result.get('model_info', 'åˆ©ç”¨ä¸å¯'))
                
                # äºˆæ¸¬çµæœã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¡¨ç¤º
                st.subheader('äºˆæ¸¬çµæœ')
                st.dataframe(result['forecast'].style.format('{:.2f}'))
                
                # äºˆæ¸¬çµæœã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                csv = result['forecast'].to_csv().encode('utf-8')
                st.download_button(
                    label=f'{model_name} äºˆæ¸¬çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰',
                    data=csv,
                    file_name=f'{model_name}_forecast_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
                    mime='text/csv'
                )
    
    # ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
    st.subheader('ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼')
    st.dataframe(df.head())
    
    # æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ
    st.subheader('æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ')
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=df.index,
        y=df[target_col],
        mode='lines',
        name=target_col
    ))
    
    if exog_cols:
        for col in exog_cols:
            # ã‚¹ã‚±ãƒ¼ãƒ«ã‚’åˆã‚ã›ã‚‹ãŸã‚æ­£è¦åŒ–
            normalized_col = (df[col] - df[col].min()) / (df[col].max() - df[col].min())
            normalized_col = normalized_col * (df[target_col].max() - df[target_col].min()) + df[target_col].min()
            
            fig.add_trace(go.Scatter(
                x=df.index,
                y=normalized_col,
                mode='lines',
                name=f'{col} (æ­£è¦åŒ–)',
                opacity=0.5
            ))
    
    fig.update_layout(
        title=f'{target_col} ã®æ™‚ç³»åˆ—',
        xaxis_title='æ—¥ä»˜',
        yaxis_title=target_col,
        legend_title='å‡¡ä¾‹',
        template='plotly_white',
        hovermode='x unified',
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±
    st.subheader('çµ±è¨ˆæƒ…å ±')
    st.dataframe(df[[target_col] + exog_cols].describe() if exog_cols else df[[target_col]].describe())
    
    # ç›¸é–¢è¡Œåˆ—
    if exog_cols:
        st.subheader('ç›¸é–¢è¡Œåˆ—')
        corr = df[[target_col] + exog_cols].corr()
        fig_corr = go.Figure(data=go.Heatmap(
            z=corr.values,
            x=corr.columns,
            y=corr.index,
            colorscale='RdBu',
            zmin=-1,
            zmax=1,
            text=corr.round(2).values,
            texttemplate='%{text}',
            textfont={"size":10}
        ))
        fig_corr.update_layout(
            title='ç›¸é–¢è¡Œåˆ—',
            width=800,
            height=800
        )
        st.plotly_chart(fig_corr, use_container_width=True)

# ã‚¢ãƒ—ãƒªã®å®Ÿè¡Œæ–¹æ³•ã‚’è¡¨ç¤º
else:
    st.info('ğŸ‘ˆ å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚')
    
    st.markdown("""
    ### ã‚¢ãƒ—ãƒªã®ä½¿ã„æ–¹
    1. å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    2. æ—¥ä»˜åˆ—ã¨ç›®çš„å¤‰æ•°åˆ—ã‚’é¸æŠ
    3. å¿…è¦ã«å¿œã˜ã¦å¤–éƒ¨å¤‰æ•°ã‚’é¸æŠ
    4. åˆ†ææœŸé–“ã¨äºˆæ¸¬æœŸé–“ã‚’è¨­å®š
    5. ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆARIMA, SARIMAX, VAR, LSTMï¼‰
    6. ã€Œãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
    
    ### å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
    - Excelãƒ•ã‚¡ã‚¤ãƒ« (.xlsx, .xls)
    
    ### æ³¨æ„äº‹é …
    - ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ã¯è‡ªå‹•çš„ã«è£œé–“ã•ã‚Œã¾ã™ãŒã€é©åˆ‡ãªå‰å‡¦ç†ã‚’æ¨å¥¨ã—ã¾ã™
    - å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆã€å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™
    - LSTMãƒ¢ãƒ‡ãƒ«ã¯ç‰¹ã«è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’æ¶ˆè²»ã—ã¾ã™
    """)
